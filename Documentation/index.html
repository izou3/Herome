<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="ECE 4180 Final Design Project Proposal  : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Herome the Stalking Zapatron</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.gatech.edu/nkoh8/ECE4180_FinalProject">View on GitHub</a>

          <h1 id="project_title">Herome the Stalking Zapatron</h1>
          <h3 id="project_tagline">By: Joshua Bredbenner, Nathanael Koh, Phuc Truong, and Ivan Zou</h3>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="description-of-design-project" class="anchor" href="#description-of-design-project" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h3>
<p>Herome is a pet robot that will follow a human owner around along with a Bluetooth interface to allow users to text audio that Herome will output and control a taser tail to ward off any potential enemies. For this project we will utilize a hardware accelerator (Coral USB Accelerator) with a USB interface to the Raspberry Pi 4B for SSD object detection. It’ll then calculate proper motor control signals to output to the Pi's PWM channels. Concurrently, a BLE chip will input user commands over UART from the Adafruit BlueTooth Lab to allow for basic on/off control and a user text input field to send text to over UART to a text-to-speech chip to output anything the user would like to say. The application is divided into the 9 sub-systems highlighted below. </p>
<p>Video Link: https://youtu.be/coP1mWwhwus</p>
<center><img src="Diagrams and Schematics/ECE4180_SystemDiagram.png" alt="SD" width="700"></center>
<center><p> Figure 1. System diagram for the robot and its subsystems. </p></center>
<center><img src="Diagrams and Schematics/ECE4180_SoftwareControlDiagram.png" alt="SCD" width="700"></center>
<center><p> Figure 2. System control diagram for the robot and its software subsystems. </p></center>
<center><img src="pictures/IMG-5168.jpg" alt="I5168" width="500"></center>
<center><img src="pictures/IMG-5169.jpg" alt="I5169" width="300"><img src="pictures/IMG-5170.jpg" alt="I5170" width="300"></center>
<center><p> Figure 3. Picture of the whole robot. </p></center>

<h3>
  <a id="human-detection" class="anchor" href="#human-detection" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting Started
</h3>
To get up and running with Herome from scratch, the following task will need to be completed. 

<h4>
  Hardware Setup
</h4>
<h6>After gathering all the parts listed in the appendix section, </h6>
<ol>
  <li>Create the schematic detailed in the Power Management section to setup the power management system for Herome. </li>
  <li>Create the schematic detailed in the Audio section to setup the bluetooth, speaker, and emic chip connection to mbed</li>
  <li>Create the schematic detailed in the Motor Control section to setup the 4 wheel chassis driven by the Pi and the hardware acceleration delegation</li>
  <li>If desired, 3D print your own decorative pieces and assemble</li>
</ol>


<h4>
  Software Setup
</h4>

<h5>Raspberry Pi Setup</h5>
<h6>The application currently uses the MobileDet V2 but any quantized model compiled using the edgetpu compiler and trained for the MS COCO dataset labels will suffice</h6>
  <ol>
    <li>Setup a version of the Debian Bullseye compatible with the user's selected Raspberry Pi</li>
    <li>Enable the CSI interface on the Raspberry Pi</li>
    <li>Clone this repo to the desired directory on the Pi
    <dl>
      <dd> git clone https://github.gatech.edu/nkoh8/ECE4180_FinalProject.git </dd>
      </dl>
    </li>
    <li>Follow these steps here at the following link to setup the TfLite and the edgetpu runtime library
      <dl>
        <dd>
          https://coral.ai/docs/edgetpu/tflite-python/
        </dd>
      </dl>
    </li>
    <li>Navigate to the /VideoProcessing directory and run
      <dl>
        <dd>pip3 install -r requirements.txt</dd>
      </dl>
    </li>
    <li>After all the dependencies are installed, run any of the following to have Herome start following you. The difference is in the execution throughput but there shouldn't be any visual noticable difference in the tracking trajectory.
      <dl>
        <dd>python3 SingleThread.py</dd>
        <dd>python3 MultiThread.py</dd>
        <dd>python3 MultiProc.py</dd>
      </dl>
    </li>
  </ol>

<h3>
<a id="human-detection" class="anchor" href="#human-detection" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Human Detection</h3>
<p>The Detection subsystem consists of the human detection task and its corresponding preprocessing requirements. An EdgeTPU chip with a lightweight model trained on the MS COCO dataset will be used to delegate SSD human detection tasks. Additionally a SORT tracker will be used to track different humans within the same frame so that Herome will continue to follow its owner even if there are other humans nearby. The human 
       detection will run within its own separate process that will stream frames at a rate of 25 fps and include other preprocessing tasks like image resize and switching color channels.</p>

 <h3>
<a id="motor-control" class="anchor" href="#motor-control" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Controller and Drive Actuator</h3>
<p>A 4-wheel differential drive with 2 dual-hbridge breakout broads acting as motor drivers is the implementation of the motor control subsystem. In terms of software execution, it will input list of objects detected from the Detection. It'll calculate the horizontal displacement from the frame's center to use as an error signal for angular velocity and the width magnitude
  of the detected bounding box as an error signal for linear velocity. We've also implemented a PD feedback control on the angular velocity and an average smoothing technique on the linear velocity to improve transients as we had issue with overshoots and oscillations due to the large momentum of Herome.
  Once we've the controlled linear and angular velocity, we applied some basic kinematics for differential drives to translate them into PWM signals for the left and right side of the wheel.</p>
<center><img src="pictures/IMG-5163.jpg" alt="I5163" width="500"></center>
<center><p> Figure 4. Motor control chips pictures for motor control subsystem. </p></center>
<center><img src="Diagrams and Schematics/ECE4180_MotorControlPinout.png" alt="I5160" width="300"><img src="Diagrams and Schematics/ECE4180_MotorControlPinout2.png" alt="I5161" width="300"></center>
<center><p> Figure 5. Pinout table for motor control subsystem and h-bridge. </p></center>
  
<h3>
<a id="power-management" class="anchor" href="#power-management" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Power Management</h3>
<p>Necessary power supply unit will be integrated within the chassis frame to power the Raspberry Pi, current to the motors, and Text to Speech chip. The mbed will be powered over USB connected from the RPi. There will be a switch to turn everything on to boot up the robot.</p>
<center><img src="Diagrams and Schematics/ECE4180_PowerSchematic.png" alt="PS" width="500"></center>
<center><p> Figure 6. Power schematic for the robot and its power subsystem. </p></center>
<p>The battery pack consists of six 3500mAh lithium ion batteries. Two sets of three batteries are wired in series to create one 7.4V 10.5Ah battery. The cells are protected by a battery management system that prevents over/under discharge and balances the battery voltages. Two 5V regulators drop the battery output voltage to 5V to power the Raspberry Pi, mBed, and the transformer. The two regulators combined can output a total of 6 amps. A voltage divider on the battery circuit allows the mBed (pin p19) to monitor the battery level and display the percentage on the front of the robot through the LEDs.</p>
<center><img src="pictures/IMG-5160.jpg" alt="I5160" width="200"><img src="pictures/IMG-5161.jpg" alt="I5161" width="200"><img src="pictures/IMG-5162.jpg" alt="I5162" width="200"></center>
<center><p> Figure 7. Power system pictures for power management subsystem. </p></center>


<h3>
<a id="text-to-speech-output" class="anchor" href="#text-to-speech-output" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Audio and Remote UI</h3>
<p>An EMIC2 Text to Speech chip with a speaker will be used to generate audio output based on text input the user enters through the Adafruit Bluetooth app. The BLE chip will receive ASCII texts and stream them serially over a UART port to the mbed that will handle the execution logic. Text inputs will be streamed over UART to the EMIC 2 chip to generate audio while button controls will adjust volume and voice settings as well as activate the taser actuator described in the next section.</p>
<center><img src="Diagrams and Schematics/ECE4180_AudioPinout.png" alt="AP" width="500"></center>
<center><p> Figure 8. Pinout table for audio subsystem and EMIC2 chip. </p></center>
<center><img src="pictures/IMG-5165.jpg" alt="I5165" width="500"></center>
<center><p> Figure 9. Audio chips pictures for text to speech output subsystem. </p></center>
<h3>
<a id="tail-taser" class="anchor" href="#tail-taser" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Taser and Tail Actuator</h3>
<p>The end of our robot will have a tail that consists of two servos and wiring for a taser. The two servos will have a tail like wagging motion that has three different states: Happy, Sad, and Nothing. The happy state will cause the tail to wag really fast with the sad state causing the tail to wag slower. The nothing state will have the tail do nothing at all and will be started when the robot does not detect a human. Additionally, there will be a taser that the user can control via our bluetooth chip. When the user presses the button 1 on the bluetooth app connected to the mbed, the taser will zap.</p>
<center><img src="pictures/IMG-5157.jpg" alt="I5157" width="200"><img src="pictures/IMG-5158.jpg" alt="I5158" width="200"><img src="pictures/IMG-5159.jpg" alt="I5159" width="200"></center>
<center><p> Figure 10. Tail pictures for tail taser subsystem. </p></center>
<h3>
<a id="chassis-mechanics-and-custom-parts" class="anchor" href="#chassis-mechanics-and-custom-parts" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mechanical Structure</h3>
<p>Majority of mechanical structure were 3D modeled and printed to support the final product. The models requires manual assembly as seen fit based on current assembly of Herome. User is encouraged to create their own models.</p>

<h3>
  <a id="related-projects-and-difference" class="anchor" href="#related-projects-and-difference" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Appendix A: Required Part</h3>

<ul>
  <li>
    1x 2S BMS
  </li>
  <li>2x 5V SMPS regulator</li>
  <li>2x SG51 servo</li>
  <li>1x High voltage transformer</li>
  <li>1x Solenoid relay</li>
  <li>1x NPN transistor</li>
  <li>
    4x resistor
  </li>
  <li>
    8x capacitor
  </li>
  <li>
    1x diode
  </li>
  <li>
    5x WS2812b LED
  </li>
  <li>
    6x Samsung 18650 3500mAh batteries
  </li>
  <li>
    2x DC motor controller
  </li>
  <li>
    4x 400 RPM DC motor
  </li>
  <li>
    1x Bluefruit module
  </li>
  <li>
    1x Text to speech module
  </li>
  <li>
    1x 4 Ohm speaker
  </li>
  <li>
    1x mBed LPC1768
  </li>
  <li>
    1x Raspberry Pi 4
  </li>
  <li>
    1x CSI camera
  </li>
  <li>
    1x Coral AI accelerator
  </li>
</ul>

<h5>Misc.</h5>

<ul>
  <li>
    XH connectors
  </li>
  <li>
    Jumper wires
  </li>
  <li>
    Single throw switch
  </li>
  <li>
    Cables
  </li>
  <li>DC barrel connector</li>

</ul>

<h3>
<a id="related-projects-and-difference" class="anchor" href="#related-projects-and-difference" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Appendix B: Closely Related Projects</h3>

<h6>Here we detail closely related projects and our improvements over them</h6>

<ul>
  <li>
    <p>https://www.instructables.com/Object-Finding-Personal-Assistant-Robot-Ft-Raspber/</p>
    <p>Uses similar hardware for object detection but the detection to motor algorithm used is overly simplified. The product is also made up of jerky and discontinuous movements that follow the human around. We’ll be improving upon the detection processing speed and the motor translation algorithm for more continuous robot tracking movements that mimic human trajectories. There will also be additional features such as Bluetooth control and audio output.</p>
  </li>

  <li>
    <p>https://www.youtube.com/watch?v=gqirgvxcXVQ&t=71s</p>
    <p>Uses a similar detection to motor translation algorithm but outputs constant motor velocities and still comprises of discontinuous movements. We’ll be improving upon this to result in a much faster processing speed as well as more continuous movements based on motor velocities proportional to error. We’ll also be adding additional features such as Bluetooth control and audio output. Most of the other robot tracking projects online are non-vision sensor based using IR or Ultra-sonic sensors instead. Of the ones that do using computer vision, they’re overly simplified and result in discontinuous movements. We’ll be improving upon these to present a much better robot follower with additional features for the user.</p>
  </li>
</ul>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">ECE 4180 Final Design Project Proposal maintained by <a href="https://github.gatech.edu/nkoh8">nkoh8</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
